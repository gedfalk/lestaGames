TF-IDF (от англ. TF — term frequency, IDF — inverse document frequency) — статистическая мера, используемая для оценки важности слова в контексте документа, являющегося частью коллекции документов или корпуса. Вес некоторого слова пропорционален частоте употребления этого слова в документе и обратно пропорционален частоте употребления слова во всех документах коллекции.

Мера TF-IDF часто используется в задачах анализа текстов и информационного поиска, например, как один из критериев релевантности документа поисковому запросу, при расчёте меры близости документов при кластеризации.
Содержание

    1 Структура формулы
    2 Числовое применение
    3 Пример
    4 Применение в модели векторного пространства
    5 См. также
    6 Примечания
    7 Литература
    8 Ссылки

Структура формулы

TF (term frequency — частота слова) — отношение числа вхождений некоторого слова к общему числу слов документа. Таким образом, оценивается важность слова t i {\displaystyle t_{i}} в пределах отдельного документа.

    t f ( t , d ) = n t ∑ k n k {\displaystyle \mathrm {tf} (t,d)={\frac {n_{t}}{\sum _{k}n_{k}}}} ,

где n t {\displaystyle n_{t}} есть число вхождений слова t {\displaystyle t} в документ, а в знаменателе — общее число слов в данном документе.

IDF (inverse document frequency — обратная частота документа) — инверсия частоты, с которой некоторое слово встречается в документах коллекции. Основоположником данной концепции является Карен Спарк Джонс[1]. Учёт IDF уменьшает вес широкоупотребительных слов. Для каждого уникального слова в пределах конкретной коллекции документов существует только одно значение IDF.

    i d f ( t , D ) = log ⁡ | D | | { d i ∈ D ∣ t ∈ d i } | {\displaystyle \mathrm {idf} (t,D)=\log {\frac {|D|}{|\{\,d_{i}\in D\mid t\in d_{i}\,\}|}}} ,[2]

где

    |D| — число документов в коллекции;
    | { d i ∈ D ∣ t ∈ d i } | {\displaystyle |\{\,d_{i}\in D\mid t\in d_{i}\,\}|} — число документов из коллекции D {\displaystyle D}, в которых встречается t {\displaystyle t} (когда n t ≠ 0 {\displaystyle n_{t}\neq 0}).

Выбор основания логарифма в формуле не имеет значения, поскольку изменение основания приводит к изменению веса каждого слова на постоянный множитель, что не влияет на соотношение весов.

Таким образом, мера TF-IDF является произведением двух сомножителей:

    t f - i d f ⁡ ( t , d , D ) = tf ⁡ ( t , d ) × idf ⁡ ( t , D ) {\displaystyle \operatorname {tf-idf} (t,d,D)=\operatorname {tf} (t,d)\times \operatorname {idf} (t,D)}

Большой вес в TF-IDF получат слова с высокой частотой в пределах конкретного документа и с низкой частотой употреблений в других документах.
Числовое применение

Существуют различные формулы, основанные на методе TF-IDF. Они отличаются коэффициентами, нормировками, использованием логарифмированных шкал.

Одной из наиболее популярных формул является формула BM25.
Пример

Если документ содержит 100 слов, и слово[3] «заяц» встречается в нём 3 раза, то частота слова (TF) для слова «заяц» в документе будет 0,03 (3/100). Вычислим IDF как десятичный логарифм отношения количества всех документов к количеству документов, содержащих слово «заяц». Таким образом, если «заяц» содержится в 1000 документах из 10 000 000 документов, то IDF будет равной: log(10 000 000/1000) = 4. Для расчета окончательного значения веса слова необходимо TF умножить на IDF. В данном примере, TF-IDF вес для слова «заяц» в выбранном документе будет равен: 0,03 × 4 = 0,12.
Применение в модели векторного пространства

Мера TF-IDF часто использует для представления документов коллекции в виде числовых векторов, отражающих важность использования каждого слова из некоторого набора слов (количество слов набора определяет размерность вектора) в каждом документе. Подобная модель называется векторной моделью и даёт возможность сравнивать тексты, сравнивая представляющие их векторы в какой-либо метрике (евклидово расстояние, косинусная мера, манхэттенское расстояние, расстояние Чебышёва и др.), то есть производя кластерный анализ.
См. также

    Закон Ципфа
    Частотность
    Российский семинар по оценке методов информационного поиска

Примечания

Jones, 2004.
В некоторых вариантах формулы не используется логарифмирование.

    Обычно перед анализом документа слова приводятся морфологическим анализатором к нормальной форме.

Литература

    Jones K. S. A statistical interpretation of term specificity and its application in retrieval (англ.) // Journal of Documentation : журнал. — MCB University: MCB University Press, 2004. — Vol. 60, no. 5. — P. 493—502. — ISSN 0022-0418.
    Солтон Дж.[англ.] Динамические библиотечно-поисковые системы. М.: — Мир, 1979.
    Salton, G. and McGill, M. J. 1983 Introduction to modern information retrieval. McGraw-Hill, ISBN 0-07-054484-0.
    Salton, G., Fox, E. A. and Wu, H. 1983 Extended Boolean information retrieval. Commun. ACM 26, 1022—1036.
    Salton, G. and Buckley, C. 1988 Term-weighting approaches in automatic text retrieval. Information Processing & Management 24(5): 513—523
    Федоровский А.Н, Костин М. Ю. Mail.ru на РОМИП-2005 // в сб. «Труды РОМИП’2005» Труды третьего российского семинара по оценке методов информационного поиска. Под ред. И. С. Некрестьянова, стр. 106—124, Санкт-Петербург: НИИ Химии СПбГУ, 2005.
    Алюнина Ю.М. Где живут чудовища? Корпусный метод обнаружения англицизмов и их производных в русскоязычном Интернете // Вестник Томского государственного университета. Филология. 2022. № 80. С. 5–29. doi: 10.17223/19986645/80/1

Ссылки

    Text Retrieval Evaluation Conference
    Cross-Language Evaluation Forum

Категории:

    Статистическая обработка естественного языкаФункции ранжирования
